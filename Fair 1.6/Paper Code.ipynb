{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d0c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samart/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fair.RCPs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfair\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRCPs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcp26, rcp45, rcp60, rcp85\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSSPs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ssp370, ssp245, ssp585\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fair.RCPs'"
     ]
    }
   ],
   "source": [
    "import fair\n",
    "import gym\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "from functools import partial\n",
    "from scipy.stats import gamma\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import fair\n",
    "from fair.RCPs import rcp26, rcp45, rcp60, rcp85\n",
    "from fair.SSPs import ssp370, ssp245, ssp585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Economic dimension\n",
    "Y = np.zeros(356+300) #global GDP\n",
    "Y_cost = np.zeros(356+300) #cost of climate change\n",
    "S = np.zeros(356+300) #renewable knowledge stock\n",
    "S[255] = 5e11 #GJ\n",
    "Y[255] = 9e13 #USD/a\n",
    "Y_cost[255] = 100*1e9 #USD/a\n",
    "S[256] = 5e11 #GJ\n",
    "Y[256] = 9e13 #USD/a\n",
    "Y_cost[256] = 100*1e9 #USD/a\n",
    "beta = 0.03 # 1/yr\n",
    "epsilon = 147. # USD/GJ\n",
    "rho = 2. # 1\n",
    "sigma = 4.e12 # GJ\n",
    "tau_S = 65. # yr\n",
    "\n",
    "\n",
    "# Labels for plots\n",
    "VARS = [\n",
    "    \"Temperature anomaly (ºC)\",\n",
    "    \"CO2 Emissions (GtC)\",\n",
    "    \"CO2 Concentration (ppm)\",\n",
    "    \"Radiative forcing (W m^-2)\",\n",
    "    \"Reward \"\n",
    "]\n",
    "\n",
    "MULTIGAS_VARS = [\n",
    "    \"Temperature anomaly (ºC)\",\n",
    "    \"CO2 Emissions (GtC)\",\n",
    "    \"CO2 Concentration (ppm)\",\n",
    "    \"CO2 forcing (W m^-2)\",\n",
    "    \"CH4 forcing (W m^-2)\",\n",
    "    \"N2O forcing (W m^-2)\",\n",
    "    \"All other well-mixed GHGs forcing (W m^-2)\",\n",
    "    \"Tropospheric O3 forcing (W m^-2)\",\n",
    "    \"Stratospheric O3 forcing (W m^-2)\",\n",
    "    \"Stratospheric water vapour from CH4 oxidation forcing (W m^-2)\",\n",
    "    \"Contrails forcing (W m^-2)\",\n",
    "    \"Aerosols forcing (W m^-2)\",\n",
    "    \"Black carbon on snow forcing (W m^-2)\",\n",
    "    \"Land use change forcing (W m^-2)\",\n",
    "    \"Volcanic forcing (W m^-2)\",\n",
    "    \"Solar forcing (W m^-2)\",\n",
    "    \"Reward \",\n",
    "]\n",
    "\n",
    "#### Reward function options ####\n",
    "def simple_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "# positive reward for temp decrease\n",
    "# negative cliff if warming exceeds 2º\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return (state[0] - cur_temp)\n",
    "\n",
    "def temp_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "# positive reward for temp under 1.5 goal\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return 1.5 - cur_temp\n",
    "\n",
    "def conc_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    # positive reward for decreased concentration\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return state[2] - cur_conc\n",
    "\n",
    "def carbon_cost_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "# impose a cost for each GtC emitted\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return -cur_emit\n",
    "\n",
    "def carbon_cost_GDP_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    reward = 0\n",
    "    if cur_temp > 2:\n",
    "        reward += -1000\n",
    "    if cur_fease < 0:\n",
    "        reward += -1000\n",
    "    if cur_fease > 0:\n",
    "        reward += -10*cur_fease\n",
    "    reward += -cur_PIB/1e11\n",
    "    return reward\n",
    "\n",
    "def temp_emit_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    # positive reward for keeping the temp under 1.5\n",
    "    # negative reward for amount of emissions reduction\n",
    "    # positive cliff for success at the end of the trial\n",
    "    # w could indicate cost\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    if t==79 and temp <=1.5:\n",
    "        return 100\n",
    "    temp = 10*(state[0] - cur_temp)\n",
    "    emit = state[1] - cur_emit\n",
    "    if cur_emit < state[1]:\n",
    "        return temp - emit\n",
    "    return temp\n",
    "\n",
    "\n",
    "def temp_emit_diff_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    # positive reward for keeping the temp under 1.5\n",
    "    # negative reward for amount of emissions reduction\n",
    "    # (reduction compared to projected amount for that year)\n",
    "    # positive cliff for success at the end of the trial\n",
    "    # w could indicate cost of emissions\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    if t==79 and temp <=1.5:\n",
    "        return 100\n",
    "    curval = t*0.6 + 36\n",
    "    temp = 10*(state[0] - cur_temp)\n",
    "    emit = curval - cur_emit\n",
    "    if cur_emit < curval:\n",
    "        return temp - emit\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7658bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03585719, 0.00285074, 0.03556975, 0.04557736, 0.34139777])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.rvs(0.2, size=5, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39c6296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create an Environment class and extend following functions:\n",
    "\n",
    "1. Constructor- initialize continuous action space and observation space, select reward function, \n",
    "    define time advancement function from FaIR. \n",
    "\n",
    "2. update_state takes in C, F, T which corresponds to emission data and changes params of self.state and self.t\n",
    "\n",
    "3. reset() sets all emissions to baseline from provided SSP scenario\n",
    "\n",
    "4. step(action) takes in action and computes forward emissions data from environment. updates state with what is computed and returns reward\n",
    "\n",
    "5. render() plots all the relevant climate info. \n",
    "\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "class Simulator(gym.Env):\n",
    "    def __init__(self, action_space=36, reward_mode=\"temp_emit_reward\", climate_params={},\n",
    "                 forcing=False, multigas=True, scenario='ssp245', verbose=1):\n",
    "        \n",
    "        # action space for the environment,\n",
    "        # the amount to increase or decrease emissions by\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            np.array([-action_space]).astype(np.float32),\n",
    "            np.array([+action_space]).astype(np.float32),\n",
    "        )\n",
    "        # state space, [temperature, carbon emissions, carbon concentration, radiative forcing]\n",
    "        if not multigas:\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                np.array([-100, -100, 0, -100]).astype(np.float32),\n",
    "                np.array([100, 100, 5000, 100]).astype(np.float32),\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = gym.spaces.Box( # both length 16\n",
    "                np.array([-100, -100, 0, -50, -50, -50, -50, -50, -50, -50, -50,\n",
    "                -50, -50, -50, -50, -50]).astype(np.float32),\n",
    "                \n",
    "                np.array([100, 100, 5000, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
    "                50, 50, 50, 50]).astype(np.float32),\n",
    "            )\n",
    "        \n",
    "        # specify the reward function to use\n",
    "        \n",
    "        self.reward_func = eval(reward_mode)\n",
    "\n",
    "        # setup climate parameters for FaIR model\n",
    "        \n",
    "        \n",
    "        if forcing:\n",
    "            climate_params['F_solar'] = 0.1 * np.sin(2 * np.pi * np.arange(736) / 11.5)\n",
    "            climate_params['F_volcanic'] = -gamma.rvs(0.2, size=736, random_state=14) # samples from gamma probability distribution\n",
    "            \n",
    "        self.forward_func = partial(fair.forward.fair_scm, **climate_params)\n",
    "        \n",
    "        self.multigas = multigas\n",
    "        self.scenario = scenario\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # set the initial state\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def update_state(self, C, F, T):\n",
    "        if self.multigas:\n",
    "            concentration = C[:,0][256+self.t]\n",
    "            forcing = np.sum(F,axis=1)[256+self.t]\n",
    "            emissions = self.emissions[:,1][256+self.t]\n",
    "        else:\n",
    "            concentration = C[256+self.t]\n",
    "            forcing = F[256+self.t]\n",
    "            emissions = self.emissions[256+self.t]\n",
    "        self.state = [T[256+self.t], emissions, concentration] + [forcing]\n",
    "        self.t += 1\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        base_emissions = eval(self.scenario).Emissions.emissions\n",
    "        if not self.multigas:\n",
    "            base_emissions = np.array([x[1]+x[2] for x in base_emissions])\n",
    "        \n",
    "        # 80 year time horizon, meet goals by 2100\n",
    "        self.emissions = base_emissions\n",
    "        # 2021 estimate of GtC of carbon emissions\n",
    "        # 2021 is the 257th year in the ssp scenario\n",
    "    \n",
    "        self.t = 0\n",
    "        # initial state\n",
    "        C, F, T = self.forward_func( #forward func computes from FaIR climate simulator, returns world\n",
    "            emissions=self.emissions,\n",
    "            useMultigas=True,\n",
    "        )\n",
    "        \n",
    "        self.update_state(C,F,T)\n",
    "        return(self.state)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        # change emissions by the action amount\n",
    "        \n",
    "        if not self.multigas:\n",
    "            # change time step to be previous time step + delta E from action (negative). Floor is zero\n",
    "            self.emissions[256+self.t] = max(self.emissions[256+self.t-1] + action[0], 0)\n",
    "        else:\n",
    "            self.emissions[:,1][256+self.t] = max(self.emissions[:,1][256+self.t-1] + action[0]*.9, 0)\n",
    "            self.emissions[:,2][256+self.t] = max(self.emissions[:,2][256+self.t-1] + action[0]*.1, 0)\n",
    "        # run FaIR simulator\n",
    "        C, F, T = self.forward_func(\n",
    "            emissions=self.emissions,\n",
    "            useMultigas= True,\n",
    "        )\n",
    "        \n",
    "        #Implementation of S, Y and Y_cost\n",
    "        \n",
    "        gamma = 1 / ( 1+(S[256+self.t-1]/sigma)**rho )\n",
    "        Y[256+self.t] = Y[256+self.t-1] + beta*Y[256+self.t-1]\n",
    "        Y_cost[256+self.t] = (10/5*T[256+self.t]-2)/100*Y[256+self.t]\n",
    "        S[256+self.t] = S[256+self.t-1] + ( (1-gamma)*Y[256+self.t-1]/epsilon - S[256+self.t-1]/tau_S )\n",
    "        # fail if temperature error\n",
    "        if math.isnan(T[256+self.t]):\n",
    "            done = True\n",
    "            \n",
    "            \n",
    "        # determine economic and other parameters to feed into loss function\n",
    "        if self.multigas:\n",
    "            cur_emit = self.emissions[:,1][256+self.t] + self.emissions[:,2][256+self.t]\n",
    "            cur_conc = C[:,0][256+self.t]\n",
    "            cur_fease = ssp_370[self.t-1] - cur_emit\n",
    "        else:\n",
    "            cur_emit = self.emissions[256+self.t]\n",
    "            cur_fease = ssp_370[self.t-1] - cur_emit\n",
    "            cur_conc = C[256+self.t]\n",
    "        cur_GDP = Y_cost[256+self.t]\n",
    "        \n",
    "        #compute the reward\n",
    "        reward = self.reward_func(self.state, T[256+self.t], self.t, cur_emit, cur_conc, cur_PIB, cur_fease)\n",
    "        \n",
    "        # update the state and info\n",
    "        self.update_state(C, F, T)\n",
    "        # end the trial once 2100 is reached\n",
    "        if self.t == 79 or self.state[0] > 4 or self.state[0] < 0: # only runs to 2100\n",
    "            done = True\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        # print the state\n",
    "        print(f'Temperature anomaly: {self.state[0]}ºC')\n",
    "        print(f'CO2 emissions: {self.state[1]} GtC')\n",
    "        print(f'CO2 concentration: {self.state[2]} ppm')\n",
    "        print(f'Radiative forcing: {self.state[3:]}')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c259ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99138ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3156edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'name' : \"test_runs\", #the path in which the run will be saved\n",
    "    \"output_path\": \"outputs\",\n",
    "    \"stdout\": False,\n",
    "    'action_space' : 36,\n",
    "    \"seed\": 158,\n",
    "    'reward_mode' : \"temp_emit_diff_reward\",\n",
    "    'forcing' : True,\n",
    "    'multigas' : False,\n",
    "    'scenario' : \"ssp245\",\n",
    "    'algorithm' : \"a2c\",\n",
    "    'learning_rate' : 2.1e-05,\n",
    "    'num_steps' : 100, \n",
    "    'gamma' : 0.9, \n",
    "    'device' : 'cpu',\n",
    "    'timestep' : 200,\n",
    "    \"n_steps\" : 5,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ab62ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mSimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclimate_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforcing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultigas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mssp245\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mSimulator.__init__\u001b[0;34m(self, action_space, reward_mode, climate_params, forcing, multigas, scenario, verbose)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m verbose\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# set the initial state\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mSimulator.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# initial state\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m C, F, T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#forward func computes from FaIR climate simulator, returns world\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43memissions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43museMultigas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_state(C,F,T)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/fair/forward.py:190\u001b[0m, in \u001b[0;36mfair_scm\u001b[0;34m(emissions, emissions_driven, C, other_rf, q, tcrecs, d, F2x, tcr_dbl, a, tau, r0, rc, rt, iirf_max, iirf_h, C_pi, E_pi, restart_in, restart_out, F_tropO3, F_aerosol, F_volcanic, F_solar, F_contrails, F_bcsnow, F_landuse, aviNOx_frac, F_ref_aviNOx, E_ref_aviNOx, F_ref_BC, E_ref_BC, fossilCH4_frac, natural, efficacy, scale, oxCH4_frac, ghg_forcing, scale_F2x, stwv_from_ch4, b_aero, b_tro3, pi_tro3, ghan_params, stevens_params, ref_isSO2, useMultigas, tropO3_forcing, ozone_feedback, lifetimes, aerosol_forcing, scaleAerosolAR5, fixPre1850RCP, useTropO3TFeedback, scaleHistoricalAR5, contrail_forcing, kerosene_supply, landuse_forcing, aCO2land, ariaci_out, bcsnow_forcing, diagnostics, gir_carbon_cycle, temperature_function, lambda_global, ocean_heat_capacity, ocean_heat_exchange, deep_ocean_efficacy)\u001b[0m\n\u001b[1;32m    188\u001b[0m     nF \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m13\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emissions_driven:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(emissions) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;129;01mor\u001b[39;00m \u001b[43memissions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m40\u001b[39m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memissions timeseries should be a nt x 40 numpy array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m     carbon_boxes_shape \u001b[38;5;241m=\u001b[39m (emissions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "env = Simulator(args['action_space'], reward_mode=args['reward_mode'], climate_params=climate, \n",
    "        forcing=False, multigas=False,scenario='ssp245', verbose=1)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c70b753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7112f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([536.49627845, 538.84369225, 541.22506941, 543.62995889,\n",
       "       546.04442051, 548.45353445, 550.84404035, 553.20679299,\n",
       "       555.5386549 , 557.84338074])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissions = np.zeros(250) # CO2 emissions in every year, nx1\n",
    "emissions[125:] = 10.0\n",
    "forcing = 0.5 * np.sin(2 * np.pi *  np.arange(250) / 14)\n",
    "C, F, T = env.forward_func(emissions=emissions, other_rf=forcing, useMultigas=False)\n",
    "C[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "663500a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4,) into shape (16,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m model_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupper())\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m model_builder(\n\u001b[1;32m     12\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     env\u001b[38;5;241m=\u001b[39mclimate_env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:203\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m: A2CSelf,\n\u001b[1;32m    191\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m A2CSelf:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m: OnPolicyAlgorithmSelf,\n\u001b[1;32m    233\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OnPolicyAlgorithmSelf:\n\u001b[1;32m    244\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 246\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:489\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:64\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     63\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:94\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4,) into shape (16,)"
     ]
    }
   ],
   "source": [
    "climate_env = Simulator(\n",
    "    action_space = args[\"action_space\"],\n",
    "    reward_mode = args['reward_mode'],\n",
    "    forcing = args['forcing'],\n",
    "    multigas = args['multigas'],\n",
    "    scenario = args['scenario']\n",
    ")\n",
    "\n",
    "model_builder = eval(args['algorithm'].upper())\n",
    "\n",
    "model = model_builder(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=climate_env,\n",
    "    learning_rate = args['learning_rate'],\n",
    "    n_steps = args['num_steps'],\n",
    "    gamma= args['gamma'],\n",
    "    verbose=1,\n",
    "    tensorboard_log=os.path.join(save_path, 'logs'),\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd8c6e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/mjy2_vc10zd7k8yny4sjghbr0000gn/T/ipykernel_12100/118988271.py:1: DeprecationWarning: Parameters `eval_env` and `eval_freq` are deprecated and will be removed in the future. Please use `EvalCallback` or a custom Callback instead.\n",
      "  model.learn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Simulator' object has no attribute 'num_envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_state_dict.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:203\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m: A2CSelf,\n\u001b[1;32m    191\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m A2CSelf:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m: OnPolicyAlgorithmSelf,\n\u001b[1;32m    233\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OnPolicyAlgorithmSelf:\n\u001b[1;32m    244\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 246\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:490\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Simulator' object has no attribute 'num_envs'"
     ]
    }
   ],
   "source": [
    "model.learn(\n",
    "    total_timesteps=args['timestep'],\n",
    "    eval_freq=20,\n",
    "    log_interval=20,\n",
    "    eval_log_path=os.path.join(save_path, 'evals')\n",
    ")\n",
    "model.save(f'{save_path}/model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370f9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training code ####\n",
    "# Output useful plots\n",
    "def make_plots(vals, args, save_path):\n",
    "    plots = MULTIGAS_VARS if args.multigas else VARS\n",
    "    \n",
    "    for i in range(len(plots)):\n",
    "        name = plots[i][:plots[i].find('(')].strip()\n",
    "        xs = [2021+x for x in range(len(vals))]\n",
    "        ys = [x[i] for x in vals]\n",
    "        plt.plot(xs, ys)\n",
    "        plt.ylabel(plots[i])\n",
    "        plt.xlabel('Year')\n",
    "        plt.savefig(os.path.join(save_path, 'plots', name))\n",
    "        plt.clf()\n",
    "    excel = pd.DataFrame({\n",
    "        \"Years\" : [2021+x for x in range(len(vals))],\n",
    "        \"Temperature anomaly\" : [x[0] for x in vals],\n",
    "        \"CO2 Emissions\" : [x[1] for x in vals],\n",
    "        \"CO2 Concentration\" : [x[2] for x in vals],\n",
    "        \"Radiative forcing\" : [x[3] for x in vals],\n",
    "        \"Reward\" : [x[4] for x in vals]\n",
    "    })\n",
    "    \n",
    "    with open(os.path.join(save_path, 'excel.xls'), 'w') as f:\n",
    "        excel.to_excel (save_path + 'excel.xls')\n",
    "        #json.dump(excel, f, indent=None, separators=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdc632",
   "metadata": {},
   "source": [
    "Learns  from stable_baselines3 import PPO, DDPG, A2C models which have model_builder and train methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4ca6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(args, save_path):\n",
    "    # Create the environment\n",
    "    env = Simulator(\n",
    "        action_space = args[\"action_space\"],\n",
    "        reward_mode = args['reward_mode'],\n",
    "        forcing = args['forcing'],\n",
    "        multigas = args['multigas'],\n",
    "        scenario = args['scenario']\n",
    "    )\n",
    "    \n",
    "    # Train the algorithm\n",
    "    model_builder = eval(args['algorithm'].upper())\n",
    "\n",
    "    \n",
    "    model = model_builder(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate = args['learning_rate'],\n",
    "        n_steps = args['num_steps'],\n",
    "        gamma= args['gamma'],\n",
    "        verbose=1,\n",
    "        device=args['device'],\n",
    "        tensorboard_log=os.path.join(save_path, 'logs'),\n",
    "    )\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=args['timestep'],\n",
    "        eval_freq=20,\n",
    "        log_interval=20,\n",
    "        eval_log_path=os.path.join(save_path, 'evals')\n",
    "    )\n",
    "    model.save(f'{save_path}/model_state_dict.pt')\n",
    "    \n",
    "    obs = env.reset()\n",
    "    vals = []\n",
    "    for i in range(80):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        vals.append(obs + [reward]) # adds reward to obs list [a,b] + [c] = [a,b,c]\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    make_plots(vals, args, save_path)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a63f5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'name' : \"test_runs\", #the path in which the run will be saved\n",
    "    \"output_path\": \"outputs\",\n",
    "    \"stdout\": False,\n",
    "    'action_space' : 36,\n",
    "    \"seed\": 158,\n",
    "    'reward_mode' : \"temp_emit_diff_reward\",\n",
    "    'forcing' : True,\n",
    "    'multigas' : True,\n",
    "    'scenario' : \"ssp245\",\n",
    "    'algorithm' : \"A2C\",\n",
    "    'learning_rate' : 2.1e-05,\n",
    "    'num_steps' : 100, \n",
    "    'gamma' : 0.9, \n",
    "    'device' : 'cpu',\n",
    "    'timestep' : 200,\n",
    "    \"n_steps\" : 5,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27fed0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directories\n",
    "def make_outdirs(args):\n",
    "    saved_path = \n",
    "    dirs = ['plots', 'logs', 'saved_models']\n",
    "    for direc in dirs:\n",
    "        path = os.path.join(save_path, direc)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f7b2a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/mjy2_vc10zd7k8yny4sjghbr0000gn/T/ipykernel_82135/3206657176.py:26: DeprecationWarning: Parameters `eval_env` and `eval_freq` are deprecated and will be removed in the future. Please use `EvalCallback` or a custom Callback instead.\n",
      "  model.learn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4,) into shape (16,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.txt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     18\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(args, file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# log total runtime and close logging file\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(args, save_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m model_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupper())\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m model_builder(\n\u001b[1;32m     16\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     tensorboard_log\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_state_dict.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:203\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m: A2CSelf,\n\u001b[1;32m    191\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m A2CSelf:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m: OnPolicyAlgorithmSelf,\n\u001b[1;32m    233\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OnPolicyAlgorithmSelf:\n\u001b[1;32m    244\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 246\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:489\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:64\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     63\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:94\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[0;34m(self, env_idx, obs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4,) into shape (16,)"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "# Setup save path and logging\n",
    "save_path = os.path.join(args['output_path'], args['name']) # outputs/test_runs/\n",
    "make_outdirs(save_path)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if not args['stdout']:\n",
    "    #sys.stdout = open(os.path.join(save_path, 'stdout.txt'), 'w')\n",
    "    file_write = open(os.path.join(save_path, 'stdout.txt'), 'w')\n",
    "    \n",
    "\n",
    "with open(os.path.join(save_path, 'config.txt'), 'w') as file:\n",
    "    json.dump(args, file, indent=2)\n",
    "\n",
    "\n",
    "run_model(args, save_path)\n",
    "\n",
    "# log total runtime and close logging file\n",
    "\n",
    "\n",
    "if not args['stdout']:\n",
    "    file_write.write(f'\\nTOTAL RUNTIME: {int((time.time() - start_time)/60.)} minutes {int((time.time() - start_time) %60)} seconds')\n",
    "    file_write.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b563b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452c252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90862e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e8165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc66372",
   "metadata": {},
   "source": [
    "# Figuring out os.path stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef360927",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc27b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "306017cc",
   "metadata": {},
   "source": [
    "# Argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060631da",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--name\", type=str, default='test', required=False)\n",
    "parser.add_argument(\"--action_space\", type=int, default=2, required=False)\n",
    "parser.add_argument(\"--reward_mode\", type=str, default='carbon_cost_GDP', required=False)\n",
    "parser.add_argument(\"--forcing\", action='store_true', required=False)\n",
    "parser.add_argument(\"--output_path\", type=str, default='outputs', required=False)\n",
    "parser.add_argument(\"--stdout\", action='store_true', required=False)\n",
    "parser.add_argument(\"--seed\", type=int, default=random.randint(1,1000), required=False)\n",
    "parser.add_argument(\"--device\", type=str, default='cpu', required=False)\n",
    "parser.add_argument(\"--lr\", type=float, default=2.1e-5, required=False)\n",
    "parser.add_argument(\"--n_steps\", type=int, default=5, required=False)\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99, required=False)\n",
    "parser.add_argument(\"--timesteps\", type=int, default=10000, required=False)\n",
    "parser.add_argument(\"--algorithm\", type=str, default='a2c', required=False)\n",
    "parser.add_argument(\"--multigas\", action='store_true', required=False)\n",
    "parser.add_argument(\"--scenario\", type=str, default='ssp245', required=False)\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b072ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training and eval loop\n",
    "def main(args, save_path):\n",
    "    # Create the environment\n",
    "    env = Simulator(\n",
    "        action_space = args.action_space,\n",
    "        reward_mode = args.reward_mode,\n",
    "        forcing = args.forcing,\n",
    "        multigas = args.multigas,\n",
    "        scenario = args.scenario\n",
    "    )\n",
    "    \n",
    "    # Train the algorithm\n",
    "    if args.algorithm == 'a2c':\n",
    "        model_builder = A2C\n",
    "    elif args.algorithm == 'ppo':\n",
    "        model_builder = PPO\n",
    "    elif args.algorithm == 'ddpg':\n",
    "        model_builder = DDPG\n",
    "    \n",
    "    model = model_builder(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate = args.lr,\n",
    "        n_steps = args.n_steps,\n",
    "        gamma=args.gamma,\n",
    "        verbose=1,\n",
    "        device=args.device,\n",
    "        tensorboard_log=os.path.join(save_path, 'logs'),\n",
    "    )\n",
    "    model.learn(\n",
    "        total_timesteps=args.timesteps,\n",
    "        eval_freq=20,\n",
    "        log_interval=20,\n",
    "        eval_log_path=os.path.join(save_path, 'evals')\n",
    "    )\n",
    "    model.save(f'{save_path}/model_state_dict.pt')\n",
    "    \n",
    "    obs = env.reset()\n",
    "    vals = []\n",
    "    for i in range(80):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        vals.append(obs + [reward])\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    make_plots(vals, args, save_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f0f2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--name\", type=str, default='test', required=False)\n",
    "parser.add_argument(\"--action_space\", type=int, default=2, required=False)\n",
    "parser.add_argument(\"--reward_mode\", type=str, default='carbon_cost_GDP', required=False)\n",
    "parser.add_argument(\"--forcing\", action='store_true', required=False)\n",
    "parser.add_argument(\"--output_path\", type=str, default='outputs', required=False)\n",
    "parser.add_argument(\"--stdout\", action='store_true', required=False)\n",
    "parser.add_argument(\"--seed\", type=int, default=random.randint(1,1000), required=False)\n",
    "parser.add_argument(\"--device\", type=str, default='cpu', required=False)\n",
    "parser.add_argument(\"--lr\", type=float, default=2.1e-5, required=False)\n",
    "parser.add_argument(\"--n_steps\", type=int, default=5, required=False)\n",
    "parser.add_argument(\"--gamma\", type=float, default=0.99, required=False)\n",
    "parser.add_argument(\"--timesteps\", type=int, default=10000, required=False)\n",
    "parser.add_argument(\"--algorithm\", type=str, default='a2c', required=False)\n",
    "parser.add_argument(\"--multigas\", action='store_true', required=False)\n",
    "parser.add_argument(\"--scenario\", type=str, default='ssp245', required=False)\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9338b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2b488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLforEE",
   "language": "python",
   "name": "mlforee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
