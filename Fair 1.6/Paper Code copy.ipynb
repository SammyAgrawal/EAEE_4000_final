{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d0c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import fair\n",
    "import gym\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from fair.RCPs import rcp26, rcp45, rcp60, rcp85\n",
    "from fair.SSPs import ssp370, ssp245, ssp585\n",
    "from functools import partial\n",
    "from scipy.stats import gamma\n",
    "from stable_baselines3 import PPO, DDPG, A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edfeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Economic dimension\n",
    "Y = np.zeros(356+300) #global GDP\n",
    "Y_cost = np.zeros(356+300) #cost of climate change\n",
    "S = np.zeros(356+300) #renewable knowledge stock\n",
    "S[255] = 5e11 #GJ\n",
    "Y[255] = 9e13 #USD/a\n",
    "Y_cost[255] = 100*1e9 #USD/a\n",
    "S[256] = 5e11 #GJ\n",
    "Y[256] = 9e13 #USD/a\n",
    "Y_cost[256] = 100*1e9 #USD/a\n",
    "beta = 0.03 # 1/yr\n",
    "epsilon = 147. # USD/GJ\n",
    "rho = 2. # 1\n",
    "sigma = 4.e12 # GJ\n",
    "tau_S = 65. # yr\n",
    "# Labels for plots\n",
    "VARS = [\n",
    "    \"Temperature anomaly (ºC)\",\n",
    "    \"CO2 Emissions (GtC)\",\n",
    "    \"CO2 Concentration (ppm)\",\n",
    "    \"Radiative forcing (W m^-2)\",\n",
    "    \"Reward \"\n",
    "]\n",
    "MULTIGAS_VARS = [\n",
    "    \"Temperature anomaly (ºC)\",\n",
    "    \"CO2 Emissions (GtC)\",\n",
    "    \"CO2 Concentration (ppm)\",\n",
    "    \"CO2 forcing (W m^-2)\",\n",
    "    \"CH4 forcing (W m^-2)\",\n",
    "    \"N2O forcing (W m^-2)\",\n",
    "    \"All other well-mixed GHGs forcing (W m^-2)\",\n",
    "    \"Tropospheric O3 forcing (W m^-2)\",\n",
    "    \"Stratospheric O3 forcing (W m^-2)\",\n",
    "    \"Stratospheric water vapour from CH4 oxidation forcing (W m^-2)\",\n",
    "    \"Contrails forcing (W m^-2)\",\n",
    "    \"Aerosols forcing (W m^-2)\",\n",
    "    \"Black carbon on snow forcing (W m^-2)\",\n",
    "    \"Land use change forcing (W m^-2)\",\n",
    "    \"Volcanic forcing (W m^-2)\",\n",
    "    \"Solar forcing (W m^-2)\",\n",
    "    \"Reward \",\n",
    "]\n",
    "\n",
    "#### Reward function options ####\n",
    "def simple_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "# positive reward for temp decrease\n",
    "# negative cliff if warming exceeds 2º\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return (state[0] - cur_temp)\n",
    "\n",
    "def temp_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "# positive reward for temp under 1.5 goal\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return 1.5 - cur_temp\n",
    "\n",
    "def conc_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    # positive reward for decreased concentration\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return state[2] - cur_conc\n",
    "\n",
    "def carbon_cost_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "# impose a cost for each GtC emitted\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return -cur_emit\n",
    "\n",
    "def carbon_cost_GDP_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    sum = 0\n",
    "    if cur_temp > 2:\n",
    "        sum += -1000\n",
    "    if cur_fease < 0:\n",
    "        sum += -1000\n",
    "    if cur_fease > 0:\n",
    "        sum += -10*cur_fease\n",
    "    sum += -cur_PIB/1e11\n",
    "    return sum\n",
    "\n",
    "def temp_emit_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    # positive reward for keeping the temp under 1.5\n",
    "    # negative reward for amount of emissions reduction\n",
    "    # positive cliff for success at the end of the trial\n",
    "    # w could indicate cost\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    if t==79 and temp <=1.5:\n",
    "        return 100\n",
    "    temp = 10*(state[0] - cur_temp)\n",
    "    emit = state[1] - cur_emit\n",
    "    if cur_emit < state[1]:\n",
    "        return temp - emit\n",
    "    return temp\n",
    "\n",
    "\n",
    "def temp_emit_diff_reward(state, cur_temp, t, cur_emit, cur_conc, cur_GDP, cur_fease):\n",
    "    # positive reward for keeping the temp under 1.5\n",
    "    # negative reward for amount of emissions reduction\n",
    "    # (reduction compared to projected amount for that year)\n",
    "    # positive cliff for success at the end of the trial\n",
    "    # w could indicate cost of emissions\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    if t==79 and temp <=1.5:\n",
    "        return 100\n",
    "    curval = t*0.6 + 36\n",
    "    temp = 10*(state[0] - cur_temp)\n",
    "    emit = curval - cur_emit\n",
    "    if cur_emit < curval:\n",
    "        return temp - emit\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c6296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create an Environment class and extend following functions:\n",
    "\n",
    "1. Constructor- initialize continuous action space and observation space, select reward function, \n",
    "    define time advancement function from FaIR. \n",
    "\n",
    "2. update_state takes in C, F, T which corresponds to emission data and changes params of self.state and self.t\n",
    "\n",
    "3. reset() sets all emissions to baseline from provided SSP scenario\n",
    "\n",
    "4. step(action) takes in action and computes forward emissions data from environment. updates state with what is computed and returns reward\n",
    "\n",
    "5. render() plots all the relevant climate info. \n",
    "\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "class Simulator(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose=1,\n",
    "        action_space=36,\n",
    "        reward_mode=\"carbon_cost_PIB\",\n",
    "        forcing=False,\n",
    "        multigas=True,\n",
    "        scenario='ssp245'\n",
    "    ):\n",
    "        # action space for the environment,\n",
    "        # the amount to increase or decrease emissions by\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            np.array([-action_space]).astype(np.float32),\n",
    "            np.array([+action_space]).astype(np.float32),\n",
    "        )\n",
    "        # state space, [temperature, carbon emissions, carbon concentration, radiative forcing]\n",
    "        if not multigas:\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                np.array([-100, -100, 0, -100]).astype(np.float32),\n",
    "                np.array([100, 100, 5000, 100]).astype(np.float32),\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                np.array([-100, -100, 0, -50, -50, -50, -50, -50, -50, -50, -50,\n",
    "                -50, -50, -50, -50, -50]).astype(np.float32),\n",
    "                \n",
    "                np.array([100, 100, 5000, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
    "                50, 50, 50, 50]).astype(np.float32),\n",
    "            )\n",
    "        \n",
    "        # specify the reward function to use\n",
    "        \n",
    "        match reward_mode:\n",
    "            case \"simple\":\n",
    "                self.reward_func = simple_reward\n",
    "            case \"temp\":\n",
    "                self.reward_func = temp_reward\n",
    "            case \"conc\":\n",
    "                self.reward_func = conc_reward\n",
    "            case \"carbon_cost\":\n",
    "                self.reward_func = carbon_cost_reward\n",
    "            case \"temp_emit\":\n",
    "                self.reward_func = temp_emit_reward\n",
    "            case \"temp_emit_diff\":\n",
    "                self.reward_func = temp_emit_diff_reward\n",
    "            case \"carbon_cost_GDP\":\n",
    "                self.reward_func = carbon_cost_GDP_reward\n",
    "\n",
    "        # setup additional forcing factors\n",
    "        # this is probably where geo engineering comes in\n",
    "        if forcing:\n",
    "            solar = 0.1 * np.sin(2 * np.pi * np.arange(736) / 11.5)\n",
    "            volcanic = -gamma.rvs(0.2, size=736, random_state=14)\n",
    "            self.forward_func = partial(fair.forward.fair_scm, F_solar=solar, F_volcanic=volcanic)\n",
    "        else:\n",
    "            self.forward_func = fair.forward.fair_scm\n",
    "        \n",
    "        self.multigas = multigas\n",
    "        self.scenario = scenario\n",
    "        # set the initial state\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def update_state(self, C, F, T):\n",
    "        if self.multigas:\n",
    "            concentration = C[:,0][256+self.t]\n",
    "            forcing = np.sum(F,axis=1)[256+self.t]\n",
    "            emissions = self.emissions[:,1][256+self.t]\n",
    "            forcing = [forcing]\n",
    "        else:\n",
    "            concentration = C[256+self.t]\n",
    "            forcing = F[256+self.t]\n",
    "            emissions = self.emissions[256+self.t]\n",
    "            forcing = [forcing]\n",
    "        self.state = [T[256+self.t], emissions, concentration] + forcing\n",
    "        self.t += 1\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        base_emissions = eval(self.scenario).Emissions.emissions\n",
    "        if not self.multigas:\n",
    "            base_emissions = np.array([x[1]+x[2] for x in base_emissions])\n",
    "        \n",
    "        # 80 year time horizon, meet goals by 2100\n",
    "        self.emissions = base_emissions\n",
    "        # 2021 estimate of GtC of carbon emissions\n",
    "        # 2021 is the 257th year in the ssp scenario\n",
    "    \n",
    "        self.t = 0\n",
    "        # initial state\n",
    "        C, F, T = self.forward_func( #forward func computes from FaIR climate simulator, returns world\n",
    "            emissions=self.emissions,\n",
    "            useMultigas=True,\n",
    "        )\n",
    "        \n",
    "        self.update_state(C,F,T)\n",
    "        return(self.state)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        # change emissions by the action amount\n",
    "        \n",
    "        if not self.multigas:\n",
    "            # change time step to be previous time step + delta E from action (negative). Floor is zero\n",
    "            self.emissions[256+self.t] = max(self.emissions[256+self.t-1] + action[0], 0)\n",
    "        else:\n",
    "            self.emissions[:,1][256+self.t] = max(self.emissions[:,1][256+self.t-1] + action[0]*.9, 0)\n",
    "            self.emissions[:,2][256+self.t] = max(self.emissions[:,2][256+self.t-1] + action[0]*.1, 0)\n",
    "        # run FaIR simulator\n",
    "        C, F, T = self.forward_func(\n",
    "            emissions=self.emissions,\n",
    "            useMultigas= True,\n",
    "        )\n",
    "        \n",
    "        #Implementation of S, Y and Y_cost\n",
    "        \n",
    "        gamma = 1 / ( 1+(S[256+self.t-1]/sigma)**rho )\n",
    "        Y[256+self.t] = Y[256+self.t-1] + beta*Y[256+self.t-1]\n",
    "        Y_cost[256+self.t] = (10/5*T[256+self.t]-2)/100*Y[256+self.t]\n",
    "        S[256+self.t] = S[256+self.t-1] + ( (1-gamma)*Y[256+self.t-1]/epsilon - S[256+self.t-1]/tau_S )\n",
    "        # fail if temperature error\n",
    "        if math.isnan(T[256+self.t]):\n",
    "            done = True\n",
    "            \n",
    "            \n",
    "        # determine economic and other parameters to feed into loss function\n",
    "        if self.multigas:\n",
    "            cur_emit = self.emissions[:,1][256+self.t] + self.emissions[:,2][256+self.t]\n",
    "            cur_conc = C[:,0][256+self.t]\n",
    "            cur_fease = ssp_370[self.t-1] - cur_emit\n",
    "        else:\n",
    "            cur_emit = self.emissions[256+self.t]\n",
    "            cur_fease = ssp_370[self.t-1] - cur_emit\n",
    "            cur_conc = C[256+self.t]\n",
    "        cur_GDP = Y_cost[256+self.t]\n",
    "        \n",
    "        #compute the reward\n",
    "        reward = self.reward_func(self.state, T[256+self.t], self.t, cur_emit, cur_conc, cur_PIB, cur_fease)\n",
    "        \n",
    "        # update the state and info\n",
    "        self.update_state(C, F, T)\n",
    "        # end the trial once 2100 is reached\n",
    "        if self.t == 79 or self.state[0] > 4 or self.state[0] < 0: # only runs to 2100\n",
    "            done = True\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        # print the state\n",
    "        print(f'Temperature anomaly: {self.state[0]}ºC')\n",
    "        print(f'CO2 emissions: {self.state[1]} GtC')\n",
    "        print(f'CO2 concentration: {self.state[2]} ppm')\n",
    "        print(f'Radiative forcing: {self.state[3:]}')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370f9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training code ####\n",
    "# Output useful plots\n",
    "def make_plots(vals, args, save_path):\n",
    "    plots = MULTIGAS_VARS if args.multigas else VARS\n",
    "    \n",
    "    for i in range(len(plots)):\n",
    "        name = plots[i][:plots[i].find('(')].strip()\n",
    "        ys = [x[i] for x in vals]\n",
    "        xs = [2021+x for x in range(len(vals))]\n",
    "        plt.plot(xs, ys)\n",
    "        plt.ylabel(plots[i])\n",
    "        plt.xlabel('Year')\n",
    "        plt.savefig(os.path.join(save_path, 'plots', name))\n",
    "        plt.clf()\n",
    "    excel = pd.DataFrame({\n",
    "        \"Years\" : [2021+x for x in range(len(vals))],\n",
    "        \"Temperature anomaly\" : [x[0] for x in vals],\n",
    "        \"CO2 Emissions\" : [x[1] for x in vals],\n",
    "        \"CO2 Concentration\" : [x[2] for x in vals],\n",
    "        \"Radiative forcing\" : [x[3] for x in vals],\n",
    "        \"Reward\" : [x[4] for x in vals]\n",
    "    })\n",
    "    \n",
    "    with open(os.path.join(save_path, 'excel.xls'), 'w') as f:\n",
    "        excel.to_excel (save_path + 'excel.xls')\n",
    "        #json.dump(excel, f, indent=None, separators=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebdc632",
   "metadata": {},
   "source": [
    "Learns  from stable_baselines3 import PPO, DDPG, A2C models which have model_builder and train methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97d657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training and eval loop\n",
    "def main(args, save_path):\n",
    "    # Create the environment\n",
    "    env = Simulator(\n",
    "        action_space = args.action_space,\n",
    "        reward_mode = args.reward_mode,\n",
    "        forcing = args.forcing,\n",
    "        multigas = args.multigas,\n",
    "        scenario = args.scenario\n",
    "    )\n",
    "    \n",
    "    # Train the algorithm\n",
    "    if args.algorithm == 'a2c':\n",
    "        model_builder = A2C\n",
    "    elif args.algorithm == 'ppo':\n",
    "        model_builder = PPO\n",
    "    elif args.algorithm == 'ddpg':\n",
    "        model_builder = DDPG\n",
    "    \n",
    "    model = model_builder(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate = args.lr,\n",
    "        n_steps = args.n_steps,\n",
    "        gamma=args.gamma,\n",
    "        verbose=1,\n",
    "        device=args.device,\n",
    "        tensorboard_log=os.path.join(save_path, 'logs'),\n",
    "    )\n",
    "    model.learn(\n",
    "        total_timesteps=args.timesteps,\n",
    "        eval_freq=20,\n",
    "        log_interval=20,\n",
    "        eval_log_path=os.path.join(save_path, 'evals')\n",
    "    )\n",
    "    model.save(f'{save_path}/model_state_dict.pt')\n",
    "    \n",
    "    obs = env.reset()\n",
    "    vals = []\n",
    "    for i in range(80):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        vals.append(obs + [reward])\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    make_plots(vals, args, save_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27fed0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directories\n",
    "def make_outdirs(save_path):\n",
    "    dirs = ['plots', 'logs']\n",
    "    for dir in dirs:\n",
    "        path = os.path.join(save_path, dir)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7b2a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--name NAME] [--action_space ACTION_SPACE]\n",
      "                             [--reward_mode REWARD_MODE] [--forcing]\n",
      "                             [--output_path OUTPUT_PATH] [--stdout]\n",
      "                             [--seed SEED] [--device DEVICE] [--lr LR]\n",
      "                             [--n_steps N_STEPS] [--gamma GAMMA]\n",
      "                             [--timesteps TIMESTEPS] [--algorithm ALGORITHM]\n",
      "                             [--multigas] [--scenario SCENARIO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/samart/Library/Jupyter/runtime/kernel-38017e09-779b-400d-a2d8-4f07c0c92eb1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samart/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--name\", type=str, default='test', required=False)\n",
    "    parser.add_argument(\"--action_space\", type=int, default=2, required=False)\n",
    "    parser.add_argument(\"--reward_mode\", type=str, default='carbon_cost_GDP', required=False)\n",
    "    parser.add_argument(\"--forcing\", action='store_true', required=False)\n",
    "    parser.add_argument(\"--output_path\", type=str, default='outputs', required=False)\n",
    "    parser.add_argument(\"--stdout\", action='store_true', required=False)\n",
    "    parser.add_argument(\"--seed\", type=int, default=random.randint(1,1000), required=False)\n",
    "    parser.add_argument(\"--device\", type=str, default='cpu', required=False)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2.1e-5, required=False)\n",
    "    parser.add_argument(\"--n_steps\", type=int, default=5, required=False)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, required=False)\n",
    "    parser.add_argument(\"--timesteps\", type=int, default=10000, required=False)\n",
    "    parser.add_argument(\"--algorithm\", type=str, default='a2c', required=False)\n",
    "    parser.add_argument(\"--multigas\", action='store_true', required=False)\n",
    "    parser.add_argument(\"--scenario\", type=str, default='ssp245', required=False)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    %tb\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    # Setup save path and logging\n",
    "    save_path = os.path.join(args.output_path, args.name)\n",
    "    make_outdirs(save_path)\n",
    "    start_time = time.time()\n",
    "    if not args.stdout:\n",
    "        sys.stdout = open(os.path.join(save_path, 'stdout.txt'), 'w')\n",
    "    with open(os.path.join(save_path, 'config.txt'), 'w') as f:\n",
    "        json.dump(args.__dict__, f, indent=2)\n",
    "    \n",
    "    \n",
    "    main(args, save_path)\n",
    "    \n",
    "    # log total runtime and close logging file\n",
    "    \n",
    "    print(f'\\nTOTAL RUNTIME: {int((time.time() - start_time)/60.)} minutes {int((time.time() - start_time) %60)} seconds')\n",
    "    if not args.stdout:\n",
    "        sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c4c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b072ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef907d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e18c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLforEE",
   "language": "python",
   "name": "mlforee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
