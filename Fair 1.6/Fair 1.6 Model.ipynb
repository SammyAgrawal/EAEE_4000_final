{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dea05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "from functools import partial\n",
    "from scipy.stats import gamma\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import fair\n",
    "from fair.RCPs import rcp26, rcp45, rcp60, rcp85\n",
    "from fair.SSPs import ssp370, ssp245, ssp585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2aba536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "736\n"
     ]
    }
   ],
   "source": [
    "current_year = 2022\n",
    "start_index = current_year - ssp245.Emissions.year[0] # 2022 - 1765\n",
    "size = ssp245.Emissions.year.size\n",
    "print(start_index)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb9eff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee1a4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "      \n",
    "        \n",
    "#Implementation of S, Y and Y_cost\n",
    "#S is renewable energy stock, Y is GDP, Y_cost is cost of climate change\n",
    "\n",
    "\n",
    "# Economic dimension\n",
    "Y = np.zeros(size) #global GDP\n",
    "Y_cost = np.zeros(size) #cost of climate change\n",
    "\n",
    "S = np.zeros(size) #renewable knowledge stock\n",
    "S[:start_index] = 5e11 #GJ\n",
    "Y[:start_index] = 9e13 #USD/a\n",
    "Y_cost[:start_index] = 1e11 #USD/a\n",
    "\n",
    "S[start_index] = 5e11 #GJ\n",
    "Y[start_index] = 9.2e13 #USD/a\n",
    "Y_cost[start_index] = 1e11 #USD/a\n",
    "\n",
    "beta = 0.03 # estimated GDP growth rate over the year\n",
    "\n",
    "# used to calculate energy stock\n",
    "epsilon = 147.0 # USD/GJ\n",
    "rho = 2.0 # 1, \n",
    "sigma = 4e12 # GJ, \n",
    "tau_S = 65.0 # yr,\n",
    "        \n",
    "\"\"\"gamma = 1 / ( 1+(S[self.t-1]/sigma)**rho )\n",
    "Y[self.t] = (1+beta) * Y[self.t-1] # calculate using assumed GDP growth rate\n",
    "Y_cost[self.t] = ((10/5*T[self.t]-2)/100)*Y[self.t] # assumed effect on GDP\n",
    "S[self.t] = S[self.t-1] + ( (1-gamma)*Y[self.t-1]/epsilon - S[self.t-1]/tau_S )\n",
    "# fail if temperature error\n",
    "if math.isnan(T[self.t]):\n",
    "    done = True\n",
    "\n",
    "\n",
    "# determine economic and other parameters to feed into loss function\n",
    "if self.multigas:\n",
    "    cur_emit = self.emissions[:,1][self.t] + self.emissions[:,2][self.t]\n",
    "    cur_conc = C[:,0][self.t]\n",
    "else:\n",
    "    cur_emit = self.emissions[self.t]\n",
    "    cur_conc = C[self.t]\n",
    "\n",
    "cur_fease = cur_emit - self.ssp_370[self.t-1]\n",
    "cur_GDP = Y[self.t] - Y_cost[self.t]\n",
    "\n",
    "#compute the reward\n",
    "\n",
    "# GDP SEEMS WRONG- what is / was PIB. Also look at PIB. \n",
    "# seem to be computing percent change in GDP, not GDP itself yet this is not how reward function is treating it. \n",
    "# RESOLVE SUS-NESS\n",
    "\n",
    "reward = self.reward_func(self.state, T[self.t], self.t, cur_emit, cur_conc, cur_GDP, Y_cost[self.t], cur_fease)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3064363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economic dimension\n",
    "Y = np.zeros(size) #global GDP\n",
    "Y_cost = np.zeros(size) #cost of climate change\n",
    "\n",
    "S = np.zeros(size) #renewable knowledge stock\n",
    "S[start_index-1] = 5e11 #GJ\n",
    "Y[start_index-1] = 9e13 #USD/a\n",
    "Y_cost[start_index-1] = 100*1e9 #USD/a\n",
    "\n",
    "S[start_index] = 5e11 #GJ\n",
    "Y[start_index] = 9.2e13 #USD/a\n",
    "Y_cost[start_index] = 100*1e9 #USD/a\n",
    "\n",
    "beta = 0.03 # estimated GDP growth rate over the year\n",
    "epsilon = 147.0 # USD/GJ\n",
    "rho = 2.0 # 1\n",
    "sigma = 4e12 # GJ\n",
    "tau_S = 65.0 # yr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ffb35d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reward function options ####\n",
    "def simple_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "# positive reward for temp decrease\n",
    "# negative cliff if warming exceeds 2ยบ\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return (state[0] - cur_temp) # punish if old temp is lower than new temp\n",
    "\n",
    "def temp_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "# positive reward for temp under 1.5 goal\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return 1.5 - cur_temp\n",
    "\n",
    "def conc_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "    # positive reward for decreased concentration\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return state[2] - cur_conc\n",
    "\n",
    "def carbon_cost_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "    # impose a cost for each GtC emitted\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    return -cur_emit\n",
    "\n",
    "def carbon_cost_GDP_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "    reward = 0\n",
    "    if cur_temp > 2:\n",
    "        reward += -1000\n",
    "    if cur_fease < 0:\n",
    "        reward += -100\n",
    "    if cur_fease > 0:\n",
    "        reward += 10*cur_fease\n",
    "    reward += cur_GDP/1e11\n",
    "    return reward\n",
    "\n",
    "def temp_emit_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "    # positive reward for keeping the temp under 1.5\n",
    "    # negative reward for amount of emissions reduction\n",
    "    # positive cliff for success at the end of the trial\n",
    "    # w could indicate cost\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    if year==2100 and temp<=1.5:\n",
    "        return 1000\n",
    "    temp = 10*(state[0] - cur_temp)\n",
    "    emit_decrease = state[1] - cur_emit # decrease in emissions\n",
    "    if emit_decrease>0:\n",
    "        return temp + emit_decrease\n",
    "    return temp\n",
    "\n",
    "\n",
    "def temp_emit_diff_reward(state, cur_temp, year, cur_emit, cur_conc, cur_GDP, GDP_cost, cur_fease):\n",
    "    # positive reward for keeping the temp under 1.5\n",
    "    # negative reward for amount of emissions reduction\n",
    "    # (reduction compared to projected amount for that year)\n",
    "    # positive cliff for success at the end of the trial\n",
    "    # w could indicate cost of emissions\n",
    "    if cur_temp > 2:\n",
    "        return -100\n",
    "    if t==79 and temp <=1.5:\n",
    "        return 100\n",
    "    curval = t*0.6 + 36\n",
    "    temp = 10*(state[0] - cur_temp)\n",
    "    emit = curval - cur_emit\n",
    "    if cur_emit < curval:\n",
    "        return temp - emit\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cace9",
   "metadata": {},
   "source": [
    "# Defining Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b5e7af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create an Environment class and extend following functions:\n",
    "\n",
    "1. Constructor- initialize continuous action space and observation space, select reward function, \n",
    "    define time advancement function from FaIR. \n",
    "\n",
    "2. update_state takes in C, F, T which corresponds to emission data and changes params of self.state and self.t\n",
    "\n",
    "3. reset() sets all emissions to baseline from provided SSP scenario\n",
    "\n",
    "4. step(action) takes in action and computes forward emissions data from environment. updates state with what is computed and returns reward\n",
    "\n",
    "5. render() plots all the relevant climate info. \n",
    "\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "class Simulator(gym.Env):\n",
    "    def __init__(self, action_space=36, reward_mode=\"temp_emit_reward\", climate_params={},\n",
    "                 forcing=False, multigas=True, scenario='ssp245', verbose=1, current_year=2022, end_year=2100):\n",
    "        \n",
    "        # action space for the environment,\n",
    "        # the amount to increase or decrease emissions by\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            np.array([-action_space]).astype(np.float32),\n",
    "            np.array([+action_space]).astype(np.float32),\n",
    "        )\n",
    "        \n",
    "        # state space, [temperature, carbon emissions, carbon concentration, radiative forcing]\n",
    "        if not multigas:\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                np.array([-100, -100, 0, -100]).astype(np.float32),\n",
    "                np.array([100, 100, 5000, 100]).astype(np.float32),\n",
    "            )\n",
    "        else:\n",
    "            self.observation_space = gym.spaces.Box( # both length 16\n",
    "                np.array([-100, -100, 0, -50, -50, -50, -50, -50, -50, -50, -50,\n",
    "                -50, -50, -50, -50, -50]).astype(np.float32),\n",
    "                \n",
    "                np.array([100, 100, 5000, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
    "                50, 50, 50, 50]).astype(np.float32),\n",
    "            )\n",
    "        \n",
    "        # specify the reward function to use\n",
    "        \n",
    "        self.reward_func = eval(reward_mode)\n",
    "\n",
    "        # setup climate parameters for FaIR model\n",
    "        \n",
    "        if forcing:\n",
    "            climate_params['F_solar'] = 0.1 * np.sin(2 * np.pi * np.arange(736) / 11.5)\n",
    "            climate_params['F_volcanic'] = -gamma.rvs(0.2, size=736, random_state=14) # samples from gamma probability distribution\n",
    "        climate_params[\"useMultigas\"] = multigas\n",
    "        self.forward_func = partial(fair.forward.fair_scm, **climate_params)\n",
    "        \n",
    "        self.multigas = multigas\n",
    "        self.scenario = eval(scenario)\n",
    "        self.verbose = verbose\n",
    "        self.forcing = forcing\n",
    "        self.state = None\n",
    "        \n",
    "        # setup time bounding\n",
    "        self.start_index = current_year - self.scenario.Emissions.year[0] # 2022 - 1765\n",
    "        self.end_timestep = end_year - self.scenario.Emissions.year[0] # # 2100 - 1765\n",
    "    \n",
    "    \n",
    "    def current_year(self):\n",
    "        return(self.scenario.Emissions.year[self.t])\n",
    "        \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        base_emissions = self.scenario.Emissions.emissions\n",
    "        ssp_370_emissions = ssp370.Emissions.emissions\n",
    "        if not self.multigas:\n",
    "            base_emissions = np.array([x[1]+x[2] for x in base_emissions])\n",
    "            ssp_370_emissions = np.array([x[1]+x[2] for x in ssp_370_emissions])\n",
    "        \n",
    "        # 80 year time horizon, meet goals by 2100\n",
    "        self.emissions = base_emissions\n",
    "        # 2022 estimate of GtC of carbon emissions\n",
    "        # 2022 is the 257th year in the ssp scenario\n",
    "        self.ssp_370 = ssp_370_emissions #for feasibility testing later\n",
    "    \n",
    "        self.t = self.start_index - 1\n",
    "        # initial state\n",
    "        C, F, T = self.forward_func( #forward func computes from FaIR climate simulator, returns world\n",
    "            emissions=self.emissions,\n",
    "        )\n",
    "        \n",
    "        self.update_state(C,F,T)\n",
    "        return(self.state)\n",
    "    \n",
    "\n",
    "        \n",
    "    def update_state(self, C, F, T):\n",
    "        \n",
    "        # state is always [temp, emission, concentration, forcing] all as scalars in given year\n",
    "        if self.multigas:\n",
    "            concentration = C[:,0][self.t]\n",
    "            forcing = np.sum(F,axis=1)[self.t]\n",
    "            emissions = self.emissions[:,1][self.t] + self.emissions[:, 2][self.t]\n",
    "        else:\n",
    "            concentration = C[self.t]\n",
    "            forcing = F[self.t]\n",
    "            emissions = self.emissions[self.t]\n",
    "        \n",
    "        self.state = [T[self.t], emissions, concentration, forcing]\n",
    "        self.t += 1\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        #should not call step without calling reset()\n",
    "        if(not self.state): #has not been initialized\n",
    "            self.reset()\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # change emissions by the action amount\n",
    "        if not self.multigas:\n",
    "            # change time step to be previous time step + delta E from action (negative). Floor is zero\n",
    "            self.emissions[self.t] = max(self.emissions[self.t-1] + action[0], 0)\n",
    "        else:\n",
    "            self.emissions[:,1][self.t] = max(self.emissions[:,1][self.t-1] + action[0]*.9, 0)\n",
    "            self.emissions[:,2][self.t] = max(self.emissions[:,2][self.t-1] + action[0]*.1, 0)\n",
    "        # run FaIR simulator\n",
    "        \n",
    "        C, F, T = self.forward_func(\n",
    "            emissions=self.emissions\n",
    "        )\n",
    "        \n",
    "        # update the state and info; keep old_state for reward later\n",
    "        old_state = self.state\n",
    "        self.update_state(C, F, T)\n",
    "        \n",
    "        # determine economic and climate parameters to feed into loss function\n",
    "        # Implementation of S, Y and Y_cost\n",
    "        #S is renewable energy stock, Y is GDP, Y_cost is cost of climate change\n",
    "        \n",
    "        gamma = 1 / ( 1+(S[self.t-1]/sigma)**rho )\n",
    "        Y[self.t] = (1 + beta) * Y[self.t-1] #\n",
    "        Y_cost[self.t] = ((10/5*T[self.t]-2)/100) * Y[self.t]\n",
    "        S[self.t] = S[self.t-1] + ( (1-gamma)*Y[self.t-1]/epsilon - S[self.t-1]/tau_S )\n",
    "        # fail if temperature error\n",
    "        if math.isnan(T[self.t]):\n",
    "            done = True\n",
    "            \n",
    "        \n",
    "        #compute the reward\n",
    "        \n",
    "        reward = self.reward_func(old_state, cur_temp, self.current_year(), cur_emit, cur_conc, cur_GDP, Y_cost[self.t], cur_fease)\n",
    "        \n",
    "        # end the trial once 2100 is reached\n",
    "        if self.t == self.end_timestep or self.state[0] > 4 or self.state[0] < 0: # only runs to 2100\n",
    "            done = True\n",
    "    \n",
    "    \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        # print the state\n",
    "        print(f\"Current Year: {self.current_year()}\")\n",
    "        print(f'Temperature anomaly: {self.state[0]}ยบC')\n",
    "        print(f'CO2 emissions: {self.state[1]} GtC')\n",
    "        print(f'CO2 concentration: {self.state[2]} ppm')\n",
    "        print(f'Radiative forcing: {self.state[3:]}')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ad37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a3601723",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'name' : \"temp_emit_a2c\",\n",
    "    'directory' : \"baby_steps\", #the path in which the run will be saved\n",
    "    'action_space' : 36,\n",
    "    \"seed\": 158,\n",
    "    'reward_mode' : \"temp_emit_diff_reward\",\n",
    "    'forcing' : True,\n",
    "    'multigas' : False,\n",
    "    'scenario' : \"ssp245\",\n",
    "    'algorithm' : \"a2c\",\n",
    "    'learning_rate' : 2.1e-05,\n",
    "    \n",
    "    'gamma' : 0.9, \n",
    "    'device' : 'cpu',\n",
    "    'iterations' : 300, \n",
    "    \"n_steps\" : 5,\n",
    "    'verbose' : 1,\n",
    "    'log_freq' : 20,\n",
    "    \n",
    "}\n",
    "\n",
    "climate = {\n",
    "    \"a\" : np.array([0.2173, 0.2240, 0.2824, 0.2763]),\n",
    "    \"tau\" : np.array([1000000, 394.4, 36.54, 4.303]),\n",
    "    \"r0\" : 32.40,\n",
    "    \"rc\" : 0.019,\n",
    "    \"rt\" : 4.165,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ecbecb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9595145294408387, 11.169595587494776, 414.0989964569708, 2.1328449551182502]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_env(env, climate_args, iters):    \n",
    "    env.reset()\n",
    "    print(\"Start Year: \", env.current_year())\n",
    "    for i in range(iters):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if(done):\n",
    "            print(\"\\n\\nSimulation concluded at iteration: \", i)\n",
    "            print(env.t, env.current_year())\n",
    "            env.render()\n",
    "            env.reset()\n",
    "    print(\"\\nFinished Testing\")\n",
    "    env.render()\n",
    "    env.reset()\n",
    "\n",
    "env = Simulator(args['action_space'], reward_mode=args['reward_mode'], climate_params=climate, \n",
    "        forcing=args['forcing'], multigas=args['multigas'], scenario=args['scenario'], verbose=args['verbose'])\n",
    "\n",
    "#test_env(env, climate, 100)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5c7e74bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((370,), (370,), (370,))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissions = np.array([x[1] + x[2] for x in ssp370.Emissions.emissions])\n",
    "\n",
    "C, F, T = env.forward_func(emissions = emissions[:370])\n",
    "C.shape, F.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1324c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cdbb7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'name' : \"temp_emit_a2c\",\n",
    "    'directory' : \"fair_1.6_basic\", #the path in which the run will be saved\n",
    "    'action_space' : 45,\n",
    "    \"seed\": 158,\n",
    "    'reward_mode' : \"carbon_cost_GDP_reward\",\n",
    "    'forcing' : True,\n",
    "    'multigas' : False,\n",
    "    'scenario' : \"ssp245\",\n",
    "    'algorithm' : \"a2c\",\n",
    "    'learning_rate' : 2.1e-04,\n",
    "    \n",
    "    'gamma' : 0.9, \n",
    "    'device' : 'cpu',\n",
    "    'iterations' : 1000, \n",
    "    \"n_steps\" : 5,\n",
    "    'verbose' : 1,\n",
    "    'log_freq' : 25,\n",
    "    \n",
    "}\n",
    "\n",
    "climate = {\n",
    "    \"a\" : np.array([0.2173, 0.2240, 0.2824, 0.2763]),\n",
    "    \"tau\" : np.array([1000000, 394.4, 36.54, 4.303]),\n",
    "    \"r0\" : 32.40,\n",
    "    \"rc\" : 0.019,\n",
    "    \"rt\" : 4.165,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "96f3079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dirs(args):\n",
    "    save_path = os.path.join(\"outputs\", args['directory'])\n",
    "    dirs = ['plots', 'logs', 'saved_models', 'evals']\n",
    "    for direc in dirs:\n",
    "        path = os.path.join(save_path, direc)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    # save args configs\n",
    "    with open(os.path.join(save_path, 'config.txt'), 'w') as file:\n",
    "        json.dump(args, file, indent=2)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def create_model(args=args, climate_args=climate, env_type=Simulator):\n",
    "    save_path = setup_dirs(args)\n",
    "    \n",
    "    env = env_type(args['action_space'], reward_mode=args['reward_mode'], climate_params=climate_args, \n",
    "        forcing=args['forcing'], multigas=args['multigas'], scenario=args['scenario'], verbose=args['verbose'])\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    model_builder = eval(args['algorithm'].upper()) # import from stable baselines\n",
    "    model = model_builder(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate = args['learning_rate'],\n",
    "        n_steps = args['n_steps'],\n",
    "        gamma= args['gamma'],\n",
    "        verbose=args['verbose'],\n",
    "        tensorboard_log=os.path.join(save_path, 'logs'),\n",
    "    )\n",
    "\n",
    "    model.set_logger(configure(\n",
    "        os.path.join(save_path, 'logs'),\n",
    "        [\"csv\", \"tensorboard\"]\n",
    "    ))\n",
    "    \n",
    "    return model, save_path\n",
    "\n",
    "def train_model(model, save_path, timesteps=args['iterations'], log_f = args['log_freq']):\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=timesteps,\n",
    "        log_interval=log_f,\n",
    "        eval_log_path=os.path.join(os.path.join(save_path, 'eval'), args['name']), #eval_freq=20,\n",
    "    )\n",
    "    \n",
    "    saved_path = os.path.join(os.path.join(save_path, 'saved_models'), args['name'])\n",
    "    model.save(saved_path)\n",
    "    \n",
    "    return saved_path, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10a056fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<stable_baselines3.a2c.a2c.A2C at 0x19d2ccf40>, 'outputs/fair_1.6_basic')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, save_path = create_model(args, climate)\n",
    "model, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0bf2717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/fair_1.6_basic/saved_models/temp_emit_a2c',\n",
       " <stable_baselines3.a2c.a2c.A2C at 0x19d2ccf40>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658b2e2",
   "metadata": {},
   "source": [
    "# Testing and Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "393d6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels for plots ##\n",
    "VARS = [\n",
    "    \"Year\",\n",
    "    \"Temperature anomaly (ยบC)\",\n",
    "    \"Emissions (GtC)\",\n",
    "    \"CO2 Concentration (ppm)\",\n",
    "    \"Radiative forcing (W / m^2)\",\n",
    "    \"Reward\"\n",
    "]\n",
    "\n",
    "MULTIGAS_VARS = [\n",
    "    \"Temperature anomaly (ยบC)\",\n",
    "    \"CO2 Emissions (GtC)\",\n",
    "    \"CO2 Concentration (ppm)\",\n",
    "    \"CO2 forcing (W m^-2)\",\n",
    "    \"CH4 forcing (W m^-2)\",\n",
    "    \"N2O forcing (W m^-2)\",\n",
    "    \"All other well-mixed GHGs forcing (W m^-2)\",\n",
    "    \"Tropospheric O3 forcing (W m^-2)\",\n",
    "    \"Stratospheric O3 forcing (W m^-2)\",\n",
    "    \"Stratospheric water vapour from CH4 oxidation forcing (W m^-2)\",\n",
    "    \"Contrails forcing (W m^-2)\",\n",
    "    \"Aerosols forcing (W m^-2)\",\n",
    "    \"Black carbon on snow forcing (W m^-2)\",\n",
    "    \"Land use change forcing (W m^-2)\",\n",
    "    \"Volcanic forcing (W m^-2)\",\n",
    "    \"Solar forcing (W m^-2)\",\n",
    "    \"Reward \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d7a958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training code ####\n",
    "\n",
    "def evaluate_model(test_iters=100, env_class=Simulator, args=args, climate_args=climate):\n",
    "    save_path = os.path.join(\"outputs\", args['directory'])\n",
    "    model_path = os.path.join(os.path.join(save_path, 'saved_models'), args['name'])\n",
    "    \n",
    "    env = env_class(args['action_space'], reward_mode=args['reward_mode'], climate_params=climate_args, \n",
    "        forcing=args['forcing'], multigas=args['multigas'], scenario=args['scenario'], verbose=args['verbose'])\n",
    "    \n",
    "    model = eval(args['algorithm'].upper()).load(model_path)\n",
    "    progress = pd.read_csv(os.path.join(save_path, \"logs/progress.csv\"))\n",
    "    \n",
    "    obs = env.reset()\n",
    "    vals = []\n",
    "    for i in range(test_iters):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        vals.append([env.current_year()] + obs + [reward]) # adds reward to obs list [a,b] + [c] = [a,b,c]\n",
    "        if((i-1)%25==0):\n",
    "            print(\"Iteration number: \", i)\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\n\\nSimulation concluded at iteration: \", i)\n",
    "            print(env.t, env.current_year())\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break;\n",
    "    if(env.close is not None):\n",
    "        env.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return model, pd.DataFrame(data=vals, columns=VARS)\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e7ed25e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1\n",
      "Iteration number:  26\n",
      "Iteration number:  51\n",
      "Iteration number:  76\n",
      "\n",
      "\n",
      "Simulation concluded at iteration:  77\n",
      "335 2100\n",
      "Current Year: 2100\n",
      "Temperature anomaly: 1.153359711631791ยบC\n",
      "CO2 emissions: 0.0 GtC\n",
      "CO2 concentration: 409.00208681961317 ppm\n",
      "Radiative forcing: [2.0665565216074375]\n"
     ]
    }
   ],
   "source": [
    "model, vals = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "596169fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Temperature anomaly (ยบC)</th>\n",
       "      <th>Emissions (GtC)</th>\n",
       "      <th>CO2 Concentration (ppm)</th>\n",
       "      <th>Radiative forcing (W / m^2)</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.975008</td>\n",
       "      <td>10.778328</td>\n",
       "      <td>416.416315</td>\n",
       "      <td>2.162714</td>\n",
       "      <td>-16.225017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.990158</td>\n",
       "      <td>10.386993</td>\n",
       "      <td>418.565154</td>\n",
       "      <td>2.190263</td>\n",
       "      <td>-22.607654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>1.004858</td>\n",
       "      <td>9.995577</td>\n",
       "      <td>420.562490</td>\n",
       "      <td>2.215743</td>\n",
       "      <td>-28.999186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026</td>\n",
       "      <td>1.019039</td>\n",
       "      <td>9.604067</td>\n",
       "      <td>422.418653</td>\n",
       "      <td>2.239314</td>\n",
       "      <td>-35.398301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2027</td>\n",
       "      <td>1.032653</td>\n",
       "      <td>9.212447</td>\n",
       "      <td>424.140238</td>\n",
       "      <td>2.261084</td>\n",
       "      <td>-41.804031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2040</td>\n",
       "      <td>1.153071</td>\n",
       "      <td>4.092651</td>\n",
       "      <td>435.339011</td>\n",
       "      <td>2.400572</td>\n",
       "      <td>-120.316745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2041</td>\n",
       "      <td>1.158027</td>\n",
       "      <td>3.696829</td>\n",
       "      <td>435.386152</td>\n",
       "      <td>2.401151</td>\n",
       "      <td>-126.114105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2042</td>\n",
       "      <td>1.162382</td>\n",
       "      <td>3.300550</td>\n",
       "      <td>435.320468</td>\n",
       "      <td>2.400344</td>\n",
       "      <td>-131.910137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2043</td>\n",
       "      <td>1.166140</td>\n",
       "      <td>2.903825</td>\n",
       "      <td>435.142539</td>\n",
       "      <td>2.398156</td>\n",
       "      <td>-137.363985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2044</td>\n",
       "      <td>1.169303</td>\n",
       "      <td>2.506675</td>\n",
       "      <td>434.852947</td>\n",
       "      <td>2.394593</td>\n",
       "      <td>-142.813840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows ร 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Temperature anomaly (ยบC)  Emissions (GtC)  CO2 Concentration (ppm)  \\\n",
       "0   2023                  0.975008        10.778328               416.416315   \n",
       "1   2024                  0.990158        10.386993               418.565154   \n",
       "2   2025                  1.004858         9.995577               420.562490   \n",
       "3   2026                  1.019039         9.604067               422.418653   \n",
       "4   2027                  1.032653         9.212447               424.140238   \n",
       "..   ...                       ...              ...                      ...   \n",
       "95  2040                  1.153071         4.092651               435.339011   \n",
       "96  2041                  1.158027         3.696829               435.386152   \n",
       "97  2042                  1.162382         3.300550               435.320468   \n",
       "98  2043                  1.166140         2.903825               435.142539   \n",
       "99  2044                  1.169303         2.506675               434.852947   \n",
       "\n",
       "    Radiative forcing (W / m^2)      Reward  \n",
       "0                      2.162714  -16.225017  \n",
       "1                      2.190263  -22.607654  \n",
       "2                      2.215743  -28.999186  \n",
       "3                      2.239314  -35.398301  \n",
       "4                      2.261084  -41.804031  \n",
       "..                          ...         ...  \n",
       "95                     2.400572 -120.316745  \n",
       "96                     2.401151 -126.114105  \n",
       "97                     2.400344 -131.910137  \n",
       "98                     2.398156 -137.363985  \n",
       "99                     2.394593 -142.813840  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "32421d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output useful plots\n",
    "def make_plots(vals, save_path):\n",
    "    for col in vals.columns[1:]:\n",
    "        xs = vals['Year']\n",
    "        ys = vals[col]\n",
    "        plt.plot(xs, ys)\n",
    "        plt.ylabel(col)\n",
    "        plt.xlabel('Year')\n",
    "        plt.savefig(os.path.join(save_path, 'plots', col))\n",
    "        plt.clf()\n",
    "    \n",
    "    vals.to_csv(os.path.join(save_path, 'saved_models','model_output.csv'))\n",
    "    #with open(os.path.join(save_path, 'saved_models','model_output.csv'), 'w') as f:\n",
    "        #json.dump(excel, f, indent=None, separators=None)make_plots(vals, args, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0449ca1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plots(vals, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0232c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Temp_Anom</th>\n",
       "      <th>Emissions</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>Forcing</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.975008</td>\n",
       "      <td>10.778328</td>\n",
       "      <td>416.416315</td>\n",
       "      <td>2.162714</td>\n",
       "      <td>-16.225017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.990158</td>\n",
       "      <td>10.386993</td>\n",
       "      <td>418.565154</td>\n",
       "      <td>2.190263</td>\n",
       "      <td>-22.607654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025</td>\n",
       "      <td>1.004858</td>\n",
       "      <td>9.995577</td>\n",
       "      <td>420.562490</td>\n",
       "      <td>2.215743</td>\n",
       "      <td>-28.999186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2026</td>\n",
       "      <td>1.019039</td>\n",
       "      <td>9.604067</td>\n",
       "      <td>422.418653</td>\n",
       "      <td>2.239314</td>\n",
       "      <td>-35.398301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2027</td>\n",
       "      <td>1.032653</td>\n",
       "      <td>9.212447</td>\n",
       "      <td>424.140238</td>\n",
       "      <td>2.261084</td>\n",
       "      <td>-41.804031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>2040</td>\n",
       "      <td>1.153071</td>\n",
       "      <td>4.092651</td>\n",
       "      <td>435.339011</td>\n",
       "      <td>2.400572</td>\n",
       "      <td>-120.316745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>2041</td>\n",
       "      <td>1.158027</td>\n",
       "      <td>3.696829</td>\n",
       "      <td>435.386152</td>\n",
       "      <td>2.401151</td>\n",
       "      <td>-126.114105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>2042</td>\n",
       "      <td>1.162382</td>\n",
       "      <td>3.300550</td>\n",
       "      <td>435.320468</td>\n",
       "      <td>2.400344</td>\n",
       "      <td>-131.910137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>2043</td>\n",
       "      <td>1.166140</td>\n",
       "      <td>2.903825</td>\n",
       "      <td>435.142539</td>\n",
       "      <td>2.398156</td>\n",
       "      <td>-137.363985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>2044</td>\n",
       "      <td>1.169303</td>\n",
       "      <td>2.506675</td>\n",
       "      <td>434.852947</td>\n",
       "      <td>2.394593</td>\n",
       "      <td>-142.813840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows ร 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Year  Temp_Anom  Emissions  Concentration   Forcing  \\\n",
       "0            0  2023   0.975008  10.778328     416.416315  2.162714   \n",
       "1            1  2024   0.990158  10.386993     418.565154  2.190263   \n",
       "2            2  2025   1.004858   9.995577     420.562490  2.215743   \n",
       "3            3  2026   1.019039   9.604067     422.418653  2.239314   \n",
       "4            4  2027   1.032653   9.212447     424.140238  2.261084   \n",
       "..         ...   ...        ...        ...            ...       ...   \n",
       "95          95  2040   1.153071   4.092651     435.339011  2.400572   \n",
       "96          96  2041   1.158027   3.696829     435.386152  2.401151   \n",
       "97          97  2042   1.162382   3.300550     435.320468  2.400344   \n",
       "98          98  2043   1.166140   2.903825     435.142539  2.398156   \n",
       "99          99  2044   1.169303   2.506675     434.852947  2.394593   \n",
       "\n",
       "        Reward  \n",
       "0   -16.225017  \n",
       "1   -22.607654  \n",
       "2   -28.999186  \n",
       "3   -35.398301  \n",
       "4   -41.804031  \n",
       "..         ...  \n",
       "95 -120.316745  \n",
       "96 -126.114105  \n",
       "97 -131.910137  \n",
       "98 -137.363985  \n",
       "99 -142.813840  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(save_path, 'saved_models','model_output.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0279a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
